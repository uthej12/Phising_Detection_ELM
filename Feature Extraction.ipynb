{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from tld import get_tld\n",
    "import whois\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import urllib, sys, bs4\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#having_IP_Address\n",
    "def ip_feature(url):\n",
    "    try:\n",
    "        url = (url.split('//')[1]).split('.')[0]\n",
    "        if url.isdigit():\n",
    "            #print('ipfeature')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('ipfeature')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('ipfeature')\n",
    "        return -1\n",
    "print(ip_feature(\"http://192.168.1.1/1QomMib\"))\n",
    "print(ip_feature(\"http://google.com//1QomMib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#URL_Length\n",
    "def url_length(url):\n",
    "    if len(url)<54:\n",
    "        #print('url length')\n",
    "        return 1\n",
    "    elif len(url)>54 and len(url)<75:\n",
    "        #print('url length')\n",
    "        return 0\n",
    "    else :\n",
    "        #print('url length')\n",
    "        return -1\n",
    "print(url_length(\"https://www.google.com/search?q=long+url&rlz=1C1CHBF_enIN794IN794&oq=long+url&aqs=chrome..69i57j0l5.2016j0j7&sourceid=chrome&ie=UTF-8\"))\n",
    "print(url_length(\"https://www.google.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#URL shortening Services\n",
    "def shortining_service(url):\n",
    "    try:\n",
    "        url = (url.split('//')[1]).split('.')[0]\n",
    "        if url=='bit' or url=='tinyurl' or url=='lc' or url=='soo' or url=='s2r':\n",
    "            #print('shortening service')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('shortening service')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('shortening service')\n",
    "        return 1\n",
    "print(shortining_service(\"http://tinyurl.com/jlg8zpc\"))\n",
    "print(shortining_service(\"https://www.youtube.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Contains @\n",
    "def having_at_symbol(url):\n",
    "    if re.findall('@',url):\n",
    "        #print('at')\n",
    "        return -1\n",
    "    else:\n",
    "        #print('at')\n",
    "        return 1\n",
    "print(having_at_symbol(\"http://tinyurl.com/jlg8@pc\"))\n",
    "print(having_at_symbol(\"http://tinyurl.com/jlg8pc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#using #\n",
    "def double_slash_redirecting(url):\n",
    "    if url.rfind('//') <7:\n",
    "        #print('#')\n",
    "        return 1\n",
    "    else:\n",
    "        #print('#')\n",
    "        return -1\n",
    "print(double_slash_redirecting(\"http://tinyurl.com/jlg8//pc\"))\n",
    "print(double_slash_redirecting(\"http://tinyurl.com/jlg8pc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#has (-) prefix and suffix\n",
    "def prefix_suffix(url):\n",
    "    try:\n",
    "        url = (url.split('.')[1]).split('.')[0]\n",
    "        if url.find('-')>0:\n",
    "            #print('prefix')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('prefix')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('prefix')\n",
    "        return -1\n",
    "print(prefix_suffix(\"http://www.Confirme-paypal.com/\"))\n",
    "print(prefix_suffix(\"http://www.Confirmepaypal.com/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Sub Domain and Multi Sub Domains\n",
    "def having_sub_domain(url):\n",
    "    try:\n",
    "        if get_tld(url,fail_silently=True) == None:\n",
    "            #print('having sub domain')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('having sub domain')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('having sub domain')\n",
    "        return -1\n",
    "print(having_sub_domain(\"http://www.hud.co.in.bc/students/\"))\n",
    "print(having_sub_domain(\"http://www.hud.co.in/students/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#HTTPS (Hyper Text Transfer Protocol with Secure Sockets Layer)\n",
    "def sslfinal_state(url):\n",
    "    try:\n",
    "        ssl = url.split(\"://\")[0]\n",
    "        url = url.split(\"://\")[1]\n",
    "        if url.find('/')>0:\n",
    "            url = url.split('/')[0]\n",
    "\n",
    "        w = whois.whois(url)\n",
    "\n",
    "        creation_date = w.creation_date\n",
    "        if isinstance(creation_date,list):\n",
    "            creation_date = creation_date[0].year\n",
    "        else:\n",
    "            creation_date = creation_date.year\n",
    "        current_date = datetime.now().year\n",
    "\n",
    "        if ssl == \"https\" and abs(creation_date - current_date) >= 2:\n",
    "            return 1\n",
    "        elif ssl == \"https\" or abs(creation_date - current_date) >= 2:\n",
    "            #print('ssl')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('ssl')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('ssl')\n",
    "        return -1\n",
    "                             \n",
    "print(sslfinal_state(\"http://www.google.com\"))\n",
    "print(sslfinal_state(\"https://www.facebook.com\"))\n",
    "print(sslfinal_state(\"http://www.hk.abchina.com/en/ebanking_4916/corporatebanking/201302/t20130204_316055.htm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Domain Registration Length\n",
    "def domain_registeration_length(url):\n",
    "    try:\n",
    "        url = url.split(\"://\")[1]\n",
    "        if url.find('/')>0:\n",
    "            url = url.split('/')[0]\n",
    "        w = whois.whois(url)\n",
    "        #print(w)\n",
    "        expiration_date = w.expiration_date\n",
    "        if isinstance(expiration_date,list):\n",
    "            expiration_date = expiration_date[0].year\n",
    "        else:\n",
    "            expiration_date = expiration_date.year\n",
    "        current_date = datetime.now().year\n",
    "        current_year = datetime.now().year\n",
    "        if expiration_date-current_year >= 1:\n",
    "            #print('domain reg length')\n",
    "            return 1\n",
    "        else:\n",
    "            #print('domain reg length')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('domain reg length')\n",
    "        return -1\n",
    "print(domain_registeration_length(\"https://www.youtube.com/watch?v=zJBIMG2Ia_g\"))\n",
    "#print(domain_registeration_length(\"http://www.hk.abchina.com/en/ebanking_4916/corporatebanking/201302/t20130204_316055.htm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Favicon\n",
    "def favicon(url):\n",
    "    #print(url)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        icon_link = soup.find(\"link\", rel=\"shortcut icon\")\n",
    "        #print(soup.prettify()[:5000])\n",
    "        if icon_link != None:\n",
    "            if icon_link['href'].find('www') >0 or icon_link['href'].find('http') >-1:\n",
    "                #print('favicon')\n",
    "                return -1\n",
    "            else:\n",
    "                #print('favcion')\n",
    "                return 1\n",
    "        else:\n",
    "            #print('favicon')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('favicon')\n",
    "        return -1\n",
    "    \n",
    "favicon(\"https://stackoverflow.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "#port\n",
    "def port(url):\n",
    "    try:\n",
    "        url = url.split('://')[1]\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)\n",
    "        result =1\n",
    "\n",
    "        #ftp\n",
    "        ftp = sock.connect_ex((url,21))\n",
    "        if ftp == 0:\n",
    "            #print('ftp open')\n",
    "            result = -1\n",
    "\n",
    "        #ssh\n",
    "        ssh = sock.connect_ex((url,22))\n",
    "        if ssh == 0:\n",
    "            #print('ssh open')\n",
    "            result = -1\n",
    "        \n",
    "        #telnet\n",
    "        telnet = sock.connect_ex((url,23))\n",
    "        if telnet == 0:\n",
    "            #print('telnet open')\n",
    "            result = -1\n",
    "        \n",
    "        #SMB\n",
    "        smb = sock.connect_ex((url,445))\n",
    "        if smb == 0:\n",
    "            #print('smb open')\n",
    "            result = -1\n",
    "        \n",
    "        #mssql\n",
    "        mssql = sock.connect_ex((url,1433))\n",
    "        if mssql == 0:\n",
    "            #print('mssql open')\n",
    "            result = -1\n",
    "        \n",
    "        #oracle\n",
    "        oracle = sock.connect_ex((url,1521))\n",
    "        if oracle == 0:\n",
    "            #print('oracle open')\n",
    "            result = -1\n",
    "        \n",
    "        #mysql\n",
    "        mysql = sock.connect_ex((url,3306))\n",
    "        if mysql == 0:\n",
    "            #print('mysql open')\n",
    "            result = -1\n",
    "        \n",
    "        #remote\n",
    "        remote = sock.connect_ex((url,3389))\n",
    "        if remote == 0:\n",
    "            #print('remote open')\n",
    "            result = -1\n",
    "\n",
    "        sock.close()\n",
    "        #print('port')\n",
    "        return result\n",
    "    \n",
    "    except:\n",
    "        #print('port')\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "print(port(\"https://www.google.com\"))\n",
    "print(port(\"http://super-a5.ru/wordpress/wp-includes/js/tinymce/plugins/tabfocus/a171025_sover01/c58752867bdcecfdb54500fe9947750a/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "#The Existence of “HTTPS” Token\n",
    "def https_token(url):\n",
    "    try:\n",
    "        url = url.split('://')[1]\n",
    "\n",
    "        if url.find('https') >-1:\n",
    "            #print('https_token')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('https_token')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('https_token')\n",
    "        return -1\n",
    "    \n",
    "print(https_token('https://www.youtube.com/watch?v=1XevJDBfx4I'))\n",
    "print(https_token('http://https-www-paypal-it-webapps-mpp-home.soft-hair.com/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Request URL\n",
    "def request_url(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        #print(soup.prettify())\n",
    "        ext = 0\n",
    "\n",
    "        imgs = soup.find_all(\"img\")\n",
    "        img_links = [x['src'] for x in imgs]\n",
    "        #print(img_links)\n",
    "        for i in img_links:\n",
    "            if i.find('http') >-1 or i.find('https')>-1:\n",
    "                ext = ext+1 \n",
    "\n",
    "        video = soup.find_all(\"video\")\n",
    "        video_links = [x['src'] for x in video]\n",
    "        #print(video_links)\n",
    "        for v in video_links:\n",
    "            if v.find('http') >-1 or v.find('https')>-1:\n",
    "                ext = ext+1\n",
    "\n",
    "        audio = soup.find_all(\"audio\")\n",
    "        audio_links = [x['src'] for x in audio]\n",
    "        #print(audio_links)\n",
    "        for a in audio_links:\n",
    "            if a.find('http') >-1 or a.find('https')>-1:\n",
    "                ext = ext+1\n",
    "\n",
    "        ext_percent = ext / (len(img_links)+len(video_links)+len(audio_links))\n",
    "        if ext_percent < 0.22:\n",
    "            #print('request url')\n",
    "            return 1\n",
    "        elif ext_percentage >=0.22 and ext_percentage < 0.61:\n",
    "            #print('request url')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('request url')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('request url')\n",
    "        return -1\n",
    "    \n",
    "request_url('https://stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#URL of Anchor\n",
    "def url_of_anchor(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('.')[1]\n",
    "        else:\n",
    "            domain = domain.split('.')[0]\n",
    "        #print(domain)\n",
    "\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        anchor = soup.find_all(\"a\", href=True)\n",
    "        anchor_links = [a['href'] for a in anchor]\n",
    "        total = len(anchor_links)\n",
    "\n",
    "        anchor_pointing_same_page = [h for h in anchor_links if h.find('#')==0]\n",
    "        hashtag = len(anchor_pointing_same_page)\n",
    "\n",
    "        anchor_to_website = [w for w in anchor_links if w.find('://')>-1 and w.find(domain) == -1 ]\n",
    "        ext_website = len(anchor_to_website)\n",
    "\n",
    "        url_of_anchor_percent = ((hashtag+ext_website)/total)*100\n",
    "\n",
    "        #print(url_of_anchor_percent)\n",
    "\n",
    "        if url_of_anchor_percent < 31:\n",
    "            #print('url of anchor')\n",
    "            return 1\n",
    "        elif url_of_anchor_percent>=31 and url_of_anchor_percent <= 67:\n",
    "            #print('url of anchor')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('url of anchor')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('url of anchor')\n",
    "        return -1\n",
    "    \n",
    "url_of_anchor('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Links in <Meta>, <Script> and <Link> tags\n",
    "def links_in_tags(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('.')[1]\n",
    "        else:\n",
    "            domain = domain.split('.')[0]\n",
    "        #print(domain)\n",
    "\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        meta = soup.find_all('meta',content=True)\n",
    "        meta_links = [m['content'] for m in meta]\n",
    "        script = soup.find_all('script',src=True)\n",
    "        script_links = [s['src'] for s in script]\n",
    "        link = soup.find_all('link', href=True)\n",
    "        link_links = [l['href'] for l in link]\n",
    "        total_links = len(meta_links)+len(script_links)+len(link_links)\n",
    "        #print(total_links)\n",
    "\n",
    "        meta_external =  [m for m in meta_links if m.find('://')>-1 and m.find(domain) == -1 ]\n",
    "        script_external =  [s for s in script_links if s.find('://')>-1 and s.find(domain) == -1 ]\n",
    "        link_external =  [l for l in link_links if l.find('://')>-1 and l.find(domain) == -1 ]\n",
    "\n",
    "        total_external = len(meta_external)+len(script_external)+len(link_external)\n",
    "        #print(total_external)\n",
    "\n",
    "        percent = (total_external/total_links)*100\n",
    "        #print(percent)\n",
    "\n",
    "        if percent<17:\n",
    "            #print('links in tag')\n",
    "            return 1\n",
    "        elif percent>=17 and percent<=81:\n",
    "            #print('links in tag')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('links in tag')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('links in tag')\n",
    "        return -1\n",
    "    \n",
    "links_in_tags('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Server Form Handler\n",
    "def sfh(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('.')[1]\n",
    "        else:\n",
    "            domain = domain.split('.')[0]\n",
    "        #print(domain)\n",
    "\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        if len(soup.find_all('form',about=True)):\n",
    "            #print('sfh')\n",
    "            return -1\n",
    "\n",
    "        form = soup.find_all('form',action=True)\n",
    "        form_link = [f['action'] for f in form]\n",
    "        res = 1\n",
    "        for link in form_link:\n",
    "            if link.find('://') >-1 and link.find('domain') == -1:\n",
    "                #print('sfh')\n",
    "                return 0\n",
    "        #print('sfh')\n",
    "        return 1\n",
    "    except:\n",
    "        #print('sfh')\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "sfh('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Submitting Information to Email\n",
    "def submitting_to_email(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        mail = soup.find_all('a',mailto=True)\n",
    "        if mail:\n",
    "            #print('submitting to email')\n",
    "            return -1\n",
    "\n",
    "        mail_script = soup.find_all('script')\n",
    "        for script in mail_script:\n",
    "            if str(script).find('mail') >-1:\n",
    "                #print('submitting to email')\n",
    "                return -1\n",
    "        #print('submitting to email')\n",
    "        return 1\n",
    "    except:\n",
    "        #print('submitting to email')\n",
    "        return -1\n",
    "\n",
    "submitting_to_email('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Abnormal URL\n",
    "def abnormal_url(url):\n",
    "    try:\n",
    "        u = url.split(\"://\")[1]\n",
    "        if u.find('/')>0:\n",
    "            u = u.split('/')[0]\n",
    "\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('www.')[1]\n",
    "            if domain.find('com/'):\n",
    "                domain = domain.split('/')[0]\n",
    "\n",
    "        #print(domain)\n",
    "\n",
    "        w = whois.whois(url)\n",
    "        #print(w)\n",
    "\n",
    "        if  type(w.domain_name) is list:\n",
    "            #print('list')\n",
    "            host = w.domain_name[0]\n",
    "        else:\n",
    "            #print('string')\n",
    "            host = w.domain_name\n",
    "\n",
    "        if host.lower() == domain.lower():\n",
    "            #print('abnormal url')\n",
    "            return 1\n",
    "        else:\n",
    "            #print('abnormal url')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('abnormal url')\n",
    "        return -1\n",
    "    \n",
    "abnormal_url('https://www.facebook.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Website Redirection\n",
    "def redirect(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('www.')[1]\n",
    "            if domain.find('com/') > -1:\n",
    "                domain = domain.split('.com')[0]\n",
    "\n",
    "        #print(domain)\n",
    "\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        meta = soup.find_all('meta',url=True)\n",
    "        if meta:\n",
    "            meta = [m['url'] for m in meta]\n",
    "        if meta:\n",
    "            meta = [m['url'] for m in meta]\n",
    "            meta = [m for m in meta if m.find('://') > -1 and m.find(domain) == -1]\n",
    "        link = soup.find_all('link',href=True)\n",
    "        #print(link)\n",
    "        if link:\n",
    "            link = [l['href'] for l in link]\n",
    "            link = [l for l in link if l.find('://') > -1 and l.find(domain) == -1]\n",
    "\n",
    "        total_length = len(meta)+len(link)\n",
    "        #print(total_length)\n",
    "        if total_length <=1:\n",
    "            #print('redirect')\n",
    "            return 1\n",
    "        elif total_length >=2 and total_length <=4:\n",
    "            #print('redirect')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('redirect')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('redirect')\n",
    "        return -1\n",
    "    \n",
    "redirect('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statusbar Customizations\n",
    "def on_mouseover(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        script = soup.find_all('script')\n",
    "        script_on_over = [s for s in script if str(s).find('onMouseOver')>-1]\n",
    "\n",
    "        if script_on_over:\n",
    "            #print('on_mouseover')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('on_mouseover')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('on_mouseover')\n",
    "        return -1\n",
    "    \n",
    "on_mouseover('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Disabling Right Click\n",
    "def rightclick(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        script = soup.find_all('script')\n",
    "        script_on_over = [s for s in script if str(s).find('onMouseOver')>-1]\n",
    "\n",
    "        if script_on_over:\n",
    "            script_event = [s for s in script if str(s).find('event.button == 2')>-1]\n",
    "            if script_event:\n",
    "                #print('rightclick')\n",
    "                return -1\n",
    "        else:\n",
    "            #print('rightclick')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('rightclick')\n",
    "        return -1\n",
    "\n",
    "rightclick('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Popup Window\n",
    "def popupwindow(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        script = soup.find_all('script')\n",
    "        script_popup = [s for s in script if str(s).find('popup')>-1]\n",
    "\n",
    "        if script_popup:\n",
    "            script_popup = [s for s in script_popup if str(s).find('input')>-1]\n",
    "            if script_popup:\n",
    "                #print('popup')\n",
    "                return -1\n",
    "        else:\n",
    "            #print('popup')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('popup')\n",
    "        return -1\n",
    "\n",
    "popupwindow('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iframe\n",
    "def iframe(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        iframe = soup.find_all('IFrame')\n",
    "        #print(iframe)\n",
    "        if iframe:\n",
    "            #print('iframe')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('iframe')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('iframe')\n",
    "        return -1\n",
    "\n",
    "iframe('https://www.stackoverflow.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Age of Domain\n",
    "def age_of_domain(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('www.')[1]\n",
    "            if domain.find('com/'):\n",
    "                domain = domain.split('/')[0]\n",
    "        #print(domain)\n",
    "\n",
    "        w = whois.whois(domain)\n",
    "        #print(w.creation_date)\n",
    "        if  type(w.creation_date) is list:\n",
    "            #print('list')\n",
    "            date = w.creation_date[0]\n",
    "        else:\n",
    "            date = w.creation_date\n",
    "\n",
    "        if (datetime.now() - date).days >183:\n",
    "            #print('age of domain')\n",
    "            return 1\n",
    "        else:\n",
    "            #print('age of domain')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('age of domain')\n",
    "        return -1\n",
    "\n",
    "age_of_domain('https://www.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DNS record\n",
    "def dnsrecord(url):\n",
    "    try:\n",
    "        domain = url.split('://')[1]\n",
    "        if domain.find('www')>-1:\n",
    "            domain = domain.split('www.')[1]\n",
    "            if domain.find('com/'):\n",
    "                domain = domain.split('/')[0]\n",
    "        #print(domain)\n",
    "\n",
    "        w = whois.whois(domain)\n",
    "        if w:\n",
    "            #print('dnsrecord')\n",
    "            return 1\n",
    "        else:\n",
    "            #print('dnsrecord')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('dnsrecord')\n",
    "        return -1\n",
    "    \n",
    "dnsrecord('https://www.facebook.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Website Traffic\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        request = 'http://data.alexa.com/data?cli=10&dat=s&url='+url+'xml'\n",
    "        #print(request)\n",
    "        rank = bs4.BeautifulSoup(urllib.request.urlopen(request).read())\n",
    "        #print(rank)\n",
    "        ans = rank.find('reach')\n",
    "        if ans:\n",
    "            ans = ans['rank']\n",
    "        #print(ans)\n",
    "\n",
    "        if int(ans) < 100000:\n",
    "            #print('web traffic in try')\n",
    "            return 1\n",
    "        elif int(ans) > 100000:\n",
    "            #print('web traffic in try')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('web traffic in try')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('web traffic except')\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "web_traffic('https://www.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PageRank\n",
    "def page_rank(url):\n",
    "    try:\n",
    "        request = 'http://data.alexa.com/data?cli=10&dat=s&url='+url+'xml'\n",
    "        #print(request)\n",
    "        rank = bs4.BeautifulSoup(urllib.request.urlopen(request).read())\n",
    "        ans = rank.find('reach')\n",
    "        if ans:\n",
    "            ans = ans['rank']\n",
    "        #print(ans)\n",
    "\n",
    "        if int(ans) < 100000:\n",
    "            #print('page rank in try')\n",
    "            return 1\n",
    "        elif int(ans) > 100000:\n",
    "            #print('page rank in try')\n",
    "            return 0\n",
    "        else:\n",
    "            #print('page rank in try')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('page rank except')\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "page_rank('https://www.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Google_index\n",
    "def google_index(url):\n",
    "    try:\n",
    "        query = {'q': 'info:' + url}\n",
    "        google = \"https://www.google.com/search?\" + urlencode(query)\n",
    "        #print(google)\n",
    "        data = requests.get(google)\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\")\n",
    "        #print(len(soup.text))\n",
    "        if len(soup.text)>10000:\n",
    "            #print('google index')\n",
    "            return 1\n",
    "        else:\n",
    "            #print('google index')\n",
    "            return -1\n",
    "    except:\n",
    "        #print('google index')\n",
    "        return -1\n",
    "    \n",
    "google_index(\"https://google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of Links\n",
    "def number_of_links(url):\n",
    "    try:\n",
    "        #print('in try')\n",
    "        r = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            #print(link.get('href'))\n",
    "            count += 1\n",
    "        #print(count)\n",
    "        if count<20:\n",
    "            #print('no of links')\n",
    "            return -1\n",
    "        else:\n",
    "            #print('no of links')\n",
    "            return 1\n",
    "    except:\n",
    "        #print('no of links')\n",
    "        return -1\n",
    "number_of_links(\"https://stackoverflow.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statistical Reports\n",
    "def statistical_report(url):\n",
    "    domain = url.split('://')[1]\n",
    "    if domain.find('www')>-1:\n",
    "        domain = domain.split('www.')[1]\n",
    "    if domain.find('com/'):\n",
    "        domain = domain.split('/')[0]\n",
    "    #print(domain)\n",
    "\n",
    "    w = whois.whois(domain)\n",
    "    if w.domain == None:\n",
    "        #print('stat report')\n",
    "        return -1\n",
    "    else:\n",
    "        #print('stat report')\n",
    "        return 1\n",
    "statistical_report(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.google.com/\"\n",
    "url_features =  [ip_feature(url), url_length(url), having_at_symbol(url),\n",
    "                prefix_suffix(url), having_sub_domain(url),\n",
    "                sslfinal_state(url), domain_registeration_length(url), favicon(url),\n",
    "                port(url), request_url(url), url_of_anchor(url), links_in_tags(url),\n",
    "                sfh(url), submitting_to_email(url), on_mouseover(url),\n",
    "                 age_of_domain(url), dnsrecord(url),\n",
    "                web_traffic(url), page_rank(url), google_index(url), number_of_links(url), statistical_report(url)]\n",
    "print(url_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "url = \"http://casas-bahiapromocoes.com/\t\"\n",
    "url_features =  [ip_feature(url), url_length(url), having_at_symbol(url),\n",
    "                prefix_suffix(url), having_sub_domain(url),\n",
    "                sslfinal_state(url), domain_registeration_length(url), favicon(url),\n",
    "                port(url), request_url(url), url_of_anchor(url), links_in_tags(url),\n",
    "                sfh(url), submitting_to_email(url), on_mouseover(url),\n",
    "                 age_of_domain(url), dnsrecord(url),\n",
    "                web_traffic(url), page_rank(url), google_index(url), number_of_links(url), statistical_report(url)]\n",
    "print(url_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
